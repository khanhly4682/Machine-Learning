Decision Tree

__1. What is Decision Tree? __
- A non-parametric supervised learning method used for classification and regression. <br>
- Decision trees learn from data with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.<br>
- Decision trees can handle both categorical and numerical data.

__2. Pros__
- Fast
- Simple
- Cheap to use, 
- Easy to understand results and it can deal with irrelevant feaures also

__3. Cons__
Prone to Overfitting.(It refers to the process when models is trained on training data too well that any noise in testing data can bring negative impacts to performance of model.)

__4. Key Factors:__
__* Entropy__
__* Information Gain__
The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain 


---
__In Nutshell__<br>
A decision tree classifier is just like a flowchart diagram with the terminal nodes representing classification outputs/decisions. Starting with a dataset, you can measure the entropy to find a way to split the set until all the data belonngs to the same class. There are several approaches to decision trees like ID3, C4.5, CART and many more. For splitting nominal valued datasets you can use the ID3 algorithm. You can use matplotlib library to visualize the tree data. Decision Trees are prone to overfitting, thus to avoid overfitting you can prune the decision tree by combining the adjacent nodes that have low information gain.
